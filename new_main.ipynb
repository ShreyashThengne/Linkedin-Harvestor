{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scraped:\n",
    "1. Honors and Awards\n",
    "2. Languages\n",
    "3. Skills\n",
    "4. Certs\n",
    "5. Educations\n",
    "6. Experiences\n",
    "7. Connections, Followers\n",
    "8. About\n",
    "9. Contact details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Edge()\n",
    "driver.get('https://www.linkedin.com/login')\n",
    "\n",
    "def login(username, pw):    \n",
    "    driver.find_element(By.ID, \"username\").send_keys(username)\n",
    "    driver.find_element(By.ID, \"password\").send_keys(pw)\n",
    "    driver.find_element(By.CLASS_NAME, \"login__form_action_container\").click()\n",
    "\n",
    "actions = ActionChains(driver)\n",
    "login('shreyashthengne@gmail.com', 'Qwerasdf@2003&')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    driver.find_element(By.ID, 'global-nav-search').click()\n",
    "    actions.send_keys(query).send_keys(Keys.ENTER).perform()\n",
    "    WebDriverWait(driver, 10).until(ec.presence_of_all_elements_located((By.CLASS_NAME, \"search-reusables__primary-filter\")))[0].click()\n",
    "    actions.send_keys(Keys.ESCAPE).perform()\n",
    "\n",
    "search('amazon sde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_soup(source_code):\n",
    "    return BeautifulSoup(source_code, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_scraper():\n",
    "    profile_links = []\n",
    "    for k in range(pages):\n",
    "        soup = create_soup(driver.page_source)\n",
    "\n",
    "        for i in soup.find_all('li', {'class':\"reusable-search__result-container\"}):\n",
    "            try:\n",
    "                profile_links.append(i.find('a',{'class':\"app-aware-link scale-down\"}).get('href'))\n",
    "            except:\n",
    "                print('Ad Encountered')\n",
    "\n",
    "        print(f\"Page {k + 1}\")\n",
    "    # try:\n",
    "        actions.send_keys(Keys.END)\n",
    "        time.sleep(2)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        WebDriverWait(driver, 10).until(ec.presence_of_element_located((By.CLASS_NAME, \"artdeco-pagination__button--next\"))).click()\n",
    "        # except:\n",
    "        #     return link_scraper()\n",
    "    return profile_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ad Encountered\n",
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "Page 4\n"
     ]
    }
   ],
   "source": [
    "profiles = link_scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(profiles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_link = driver.current_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.linkedin.com/in/abhishek-paithane-a617a3119/'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the soup ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the collection of all the cards on linkedin profile (like about section, activity section, experience, etc)\n",
    "soup = create_soup(driver.page_source)\n",
    "cards = soup.find_all('section', {\"class\": \"artdeco-card pv-profile-card break-words mt2\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contact_data():\n",
    "    try:\n",
    "        WebDriverWait(driver,10)\n",
    "        intro_card = soup.find('section', {'class':'artdeco-card TIIKQjPaJzLNuejCihgbgRkECKXApRs'})\n",
    "        name = intro_card.find('a', {'class':'ember-view pv-text-details__about-this-profile-entrypoint'}).text.strip()\n",
    "\n",
    "        driver.find_element(By.ID, \"top-card-text-details-contact-info\").click()\n",
    "        obj_list = WebDriverWait(driver, 10).until(ec.presence_of_all_elements_located((By.CLASS_NAME, \"pv-contact-info__contact-type\")))\n",
    "        contact_info = []\n",
    "        for sel_obj in obj_list:\n",
    "            contact_info.append(sel_obj.text.split(\"\\n\"))\n",
    "        try:\n",
    "            contact_info.remove(['Birthday'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        def nested_list_to_dict(l):\n",
    "            d = {}\n",
    "            for i in l:\n",
    "                d[i[0]] = i[1]\n",
    "            return d\n",
    "\n",
    "        actions = ActionChains(driver)\n",
    "        actions.send_keys(Keys.ESCAPE).perform()\n",
    "        contact_info_dict = nested_list_to_dict(contact_info)\n",
    "        contact_info_dict['name'] = name\n",
    "\n",
    "        if contact_info_dict == []:\n",
    "            contact_info_dict = get_contact_data()\n",
    "        \n",
    "        contact_info_dict['profile_link'] = contact_info_dict[contact_info_dict['name'].split(\" \")[0] + \"’s \" + 'Profile']\n",
    "        contact_info_dict.pop(contact_info_dict['name'].split(\" \")[0] + \"’s \" + 'Profile')\n",
    "            \n",
    "        for field in ['Website', 'Email', 'IM','Birthday','Connected', 'Address', 'Phone', 'name', 'profile_link']:\n",
    "            if field not in contact_info_dict:\n",
    "                contact_info_dict[field] = None\n",
    "        return contact_info_dict\n",
    "    \n",
    "    except:\n",
    "        driver.refresh()\n",
    "        return get_contact_data()\n",
    "    \n",
    "contacts = get_contact_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections_available = []\n",
    "for i in cards:\n",
    "    sections_available.append(i.find().get('id'))\n",
    "# sections_available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connections, followers, about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections = soup.find('div', {'class', \"mt2 relative\"}).findNextSibling().text.strip()\n",
    "for i in cards:\n",
    "    if i.find('div', {'id':'content_collections'}) != None:\n",
    "        followers = i.find('div', {'class':'pvs-header__title-container'}).text.strip().split()[1]\n",
    "        break\n",
    "\n",
    "\n",
    "for i in cards:\n",
    "    if i.find('div', {'id':'about'}) != None:\n",
    "        about = i.find('div', {'class':\"display-flex ph5 pv3\"}).text.strip()\n",
    "        break\n",
    "\n",
    "about = about[:len(about)//2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experience():\n",
    "    try:\n",
    "        driver.get(profile_link + 'details/experience/')\n",
    "        WebDriverWait(driver, 10)\n",
    "        soup = create_soup(driver.page_source)\n",
    "\n",
    "        all_exp = soup.find_all('li', {'class', 'pvs-list__paged-list-item artdeco-list__item pvs-list__item--line-separated pvs-list__item--one-column'})\n",
    "\n",
    "        def extract_exp_data(sample):\n",
    "            '''\n",
    "            role, company, job_type, from, to, duration\n",
    "            '''\n",
    "            \n",
    "            role = sample.find('div', {'class', 'display-flex flex-wrap align-items-center full-height'}).text.strip()\n",
    "            role = role[:len(role)//2]\n",
    "\n",
    "            company_and_job_type = sample.find('span', {'class', 't-14 t-normal'}).text.strip()\n",
    "            company_and_job_type = company_and_job_type[:len(company_and_job_type)//2].split(\" · \")\n",
    "            company = company_and_job_type[0]\n",
    "            \n",
    "            try:\n",
    "                job_type = company_and_job_type[1]\n",
    "            except:\n",
    "                job_type = None\n",
    "\n",
    "            period, duration = sample.find('span', {'class', \"pvs-entity__caption-wrapper\"}).text.split(' · ')\n",
    "            period = period.split(\" - \")\n",
    "            \n",
    "            try:\n",
    "                duration = int(duration.split(\" \")[0]) * 12 + int(duration.split(\" \")[2])\n",
    "            except:\n",
    "                duration = int(duration.split(\" \")[0])\n",
    "\n",
    "            try:\n",
    "                period = {'from': datetime.strptime(period[0], \"%b %Y\"), 'to': datetime.strptime(period[1], \"%b %Y\")}\n",
    "            except:\n",
    "                period = {'from': datetime.strptime(period[0], \"%b %Y\"), 'to': datetime.strptime(datetime.now().strftime(\"%b %Y\"), \"%b %Y\")}\n",
    "            return {'role':role, 'company':company, 'job_type':job_type, 'from':period['from'], 'to':period['to'], 'duration':duration}\n",
    "\n",
    "        all_experiences = []\n",
    "        for i in  all_exp:\n",
    "            all_experiences.append(extract_exp_data(i))\n",
    "\n",
    "        if all_experiences == []:\n",
    "            all_experiences = get_experience()\n",
    "\n",
    "        return all_experiences\n",
    "    except:\n",
    "        return get_experience()\n",
    "if 'experience' in sections_available:\n",
    "    experiences = get_experience()\n",
    "else:\n",
    "    experiences = [{\n",
    "        'role':None,\n",
    "        'company': None,\n",
    "        'job_type': None,\n",
    "        'from': None,\n",
    "        'to': None,\n",
    "        'duration': None\n",
    "    }]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Educations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_education():\n",
    "    try:\n",
    "        driver.get(profile_link + 'details/education/')\n",
    "        WebDriverWait(driver, 10)\n",
    "        soup = create_soup(driver.page_source)\n",
    "\n",
    "        all_edu = soup.find_all('li', {'class', 'pvs-list__paged-list-item artdeco-list__item pvs-list__item--line-separated pvs-list__item--one-column'})\n",
    "\n",
    "        def extract_education_data(sample):\n",
    "            '''\n",
    "            college, degree, period\n",
    "            '''\n",
    "            college = sample.find('div', {'class', 'display-flex flex-wrap align-items-center full-height'}).text.strip()\n",
    "            college = college[:len(college)//2]\n",
    "            degree = sample.find('span', {'class', 't-14 t-normal'}).text.strip()\n",
    "            degree = degree[:len(degree)//2]\n",
    "            period = sample.find('span', {'class', 'pvs-entity__caption-wrapper'}).text.strip().split(\" - \")\n",
    "            period = {'from': period[0], 'to': period[1]}\n",
    "            return {'college':college, 'degree':degree, 'from':period['from'], 'to':period['to']}\n",
    "\n",
    "        all_education = []\n",
    "        for i in  all_edu:\n",
    "            all_education.append(extract_education_data(i))\n",
    "        \n",
    "        if all_education == []:\n",
    "            all_education = get_education()\n",
    "        \n",
    "        return all_education\n",
    "    except:\n",
    "        return get_education()\n",
    "if 'education' in sections_available:\n",
    "    education = get_education()\n",
    "else:\n",
    "    education = [{\n",
    "        'college': None,\n",
    "        'degree': None,\n",
    "        'from': None,\n",
    "        'to': None\n",
    "    }]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Certifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_certifications():\n",
    "    try:\n",
    "        driver.get(profile_link + 'details/certifications/')\n",
    "        WebDriverWait(driver, 10)\n",
    "        soup = create_soup(driver.page_source)\n",
    "\n",
    "        def extract_certifications(sample):\n",
    "            '''\n",
    "            name, org, issued, link\n",
    "            '''\n",
    "            sample = all_certs[0]\n",
    "            name = sample.find('div', {'class', 'display-flex flex-wrap align-items-center full-height'}).text.strip()\n",
    "            organization = sample.find('span', {'class', 't-14 t-normal'}).text\n",
    "            organization = organization[:len(organization)//2]\n",
    "            issued = datetime.strptime(sample.find('span', {'class', 'pvs-entity__caption-wrapper'}).text.split(\"Issued\")[1].strip(), '%b %Y')\n",
    "            link = sample.find('a', {'class', 'optional-action-target-wrapper artdeco-button artdeco-button--secondary artdeco-button--standard artdeco-button--2 artdeco-button--muted inline-flex justify-center align-self-flex-start button-placement-wrap'}).get('href')\n",
    "            return {'cert_name':name, 'cert_org': organization, 'cert_issued':issued, 'cert_link':link}\n",
    "\n",
    "        all_certs = soup.find_all('li', {'class', 'pvs-list__paged-list-item artdeco-list__item pvs-list__item--line-separated pvs-list__item--one-column'})\n",
    "\n",
    "        all_certification = []\n",
    "        for i in  all_certs:\n",
    "            all_certification.append(extract_certifications(i))\n",
    "\n",
    "        if all_certification == []:\n",
    "            all_certification = get_certifications()\n",
    "\n",
    "        return all_certification\n",
    "    except:\n",
    "        return get_certifications()\n",
    "\n",
    "if 'licenses_and_certifications' in sections_available:\n",
    "    certifications = get_certifications()\n",
    "else:\n",
    "    certifications = [{\n",
    "        'cert_name': None,\n",
    "        'cert_org': None,\n",
    "        'cert_issued': None,\n",
    "        'cert_link': None\n",
    "  }]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skills():\n",
    "    try:\n",
    "        driver.get(profile_link + 'details/skills/')\n",
    "        WebDriverWait(driver, 10)\n",
    "        soup = create_soup(driver.page_source)\n",
    "\n",
    "        all_skills = soup.find_all('li', {'class', 'pvs-list__paged-list-item artdeco-list__item pvs-list__item--line-separated pvs-list__item--one-column'})\n",
    "\n",
    "        def extract_skills(sample):\n",
    "            '''\n",
    "            skill\n",
    "            '''\n",
    "            skill = sample.find('div', {'class':\"display-flex flex-wrap align-items-center full-height\"}).text.strip()\n",
    "            skill = skill[:len(skill)//2]\n",
    "            return skill\n",
    "\n",
    "        skills = set()\n",
    "        for i in all_skills:\n",
    "            skills.add(extract_skills(i))\n",
    "        skills = list(skills)\n",
    "\n",
    "        if skills == []:\n",
    "            skills = get_skills()\n",
    "        \n",
    "        return skills\n",
    "    except:\n",
    "        return get_skills()\n",
    "\n",
    "if 'skills' in sections_available:\n",
    "    skills = get_skills()\n",
    "else:\n",
    "    skills = [None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_languages():\n",
    "    try:\n",
    "        driver.get(profile_link + 'details/languages/')\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        soup = create_soup(driver.page_source)\n",
    "\n",
    "        all_langs = soup.find_all('li', {'class', 'pvs-list__paged-list-item artdeco-list__item pvs-list__item--line-separated pvs-list__item--one-column'})\n",
    "\n",
    "        def extract_langs(sample):\n",
    "            '''\n",
    "            languages, profiency\n",
    "            '''\n",
    "            lang = sample.find('div', {'class':\"display-flex flex-wrap align-items-center full-height\"}).text.strip()\n",
    "            lang = lang[:len(lang)//2]\n",
    "            profiency = sample.find('span', {'class':\"t-14 t-normal t-black--light\"}).text.strip()\n",
    "            profiency = profiency[:len(profiency)//2]\n",
    "\n",
    "            return {'language':lang, 'profiency':profiency}\n",
    "\n",
    "        langs = []\n",
    "        for i in all_langs:\n",
    "            langs.append(extract_langs(i))\n",
    "        \n",
    "        if langs == []:\n",
    "            langs = get_languages()\n",
    "        return langs\n",
    "    except:\n",
    "        return get_languages\n",
    "    \n",
    "if 'languages' in sections_available:\n",
    "    languages = get_languages()\n",
    "else:\n",
    "    languages = [{\n",
    "        'language': None,\n",
    "        'profiency': None\n",
    "    }]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Honors and Awards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_awards():\n",
    "    try:\n",
    "        driver.get(profile_link + 'details/honors/')\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        soup = create_soup(driver.page_source)\n",
    "\n",
    "        all_awards = soup.find_all('li', {'class', 'pvs-list__paged-list-item artdeco-list__item pvs-list__item--line-separated pvs-list__item--one-column'})\n",
    "\n",
    "        def extract_langs(sample):\n",
    "            '''\n",
    "            award_name, org, date\n",
    "            '''\n",
    "\n",
    "            award_name = sample.find('div', {'class':\"display-flex flex-wrap align-items-center full-height\"}).text.strip()\n",
    "            award_name = award_name[:len(award_name)//2]\n",
    "            org_and_date = sample.find('span', {'class':\"t-14 t-normal\"}).text.strip()\n",
    "            org_and_date = org_and_date[:len(org_and_date)//2].split(' · ')\n",
    "            org = org_and_date[0]\n",
    "            date = datetime.strptime(org_and_date[1], '%b %Y')\n",
    "            return {'award_name':award_name, 'org':org, 'date':date}\n",
    "\n",
    "\n",
    "        awards = []\n",
    "        for i in all_awards:\n",
    "            awards.append(extract_langs(i))\n",
    "        \n",
    "        if awards == []:\n",
    "            awards = get_awards()\n",
    "        return awards\n",
    "    except:\n",
    "        return get_awards\n",
    "\n",
    "if 'honors_and_awards' in sections_available:\n",
    "    awards = get_awards()\n",
    "else:\n",
    "    awards = [{\n",
    "        'award_name': None,\n",
    "        'org': None,\n",
    "        'date': None\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linkedin_scraper_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
